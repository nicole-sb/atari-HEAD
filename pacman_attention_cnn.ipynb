{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pacman_attention_cnn",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicole-sb/atari-HEAD/blob/main/pacman_attention_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NJZ7nNC89hB",
        "outputId": "9d138739-5022-4907-fb11-7466040ec4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "EeYHyjcjpp6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_path = \"/content/drive/MyDrive/ErdosBootcampProject\""
      ],
      "metadata": {
        "id": "XKhRJ2Vcclte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"{}/raw_data/highscore\".format(parent_path)"
      ],
      "metadata": {
        "id": "TuYsadRIplNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tars = glob.glob(\"{}/*.tar.bz2\".format(train_path))\n",
        "tars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_64I_SeczXfg",
        "outputId": "3d9cec2b-0761-4dfe-a4ad-f63ca5b04d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/ErdosBootcampProject/raw_data/highscore/118_RZ_4303947_Sep-01-17-15-39.tar.bz2',\n",
              " '/content/drive/MyDrive/ErdosBootcampProject/raw_data/highscore/593_RZ_5037271_Aug-05-15-35-12.tar.bz2']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xjf {tars[0]}"
      ],
      "metadata": {
        "id": "al_5EQEeza4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_data = pd.read_csv(\"{}/raw_data/combined.csv\".format(parent_path))\n",
        "data_len = int(meta_data.loc[1764212, 'frame_id'].split('_')[-1])\n",
        "tar_name = tars[0].split('/')[-1].split('.tar')[0]\n",
        "prefix = \"RZ_4303947_\"\n",
        "def load_img(index):\n",
        "  return Image.open(\"{}/{}{}.png\".format(tar_name, prefix, index+1)).convert('RGB')\n",
        "\n",
        "#Read entire dataset into memory\n",
        "X_train = [np.array(load_img(i)) for i in range(data_len)]"
      ],
      "metadata": {
        "id": "VxXhUDdlCUYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_data.loc[1764212, 'frame_id'].split('_')[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "60sA3rH8PS3m",
        "outputId": "cada540d-1431-4085-e214-671010f6410c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RZ_5037271_17381'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_img(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "356pQGuTO27R",
        "outputId": "2665aed1-7041-471d-e99f-bae6eb06903d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=160x210 at 0x7FBA502C5C10>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAAFbElEQVR4nO3dMWocSRTG8dYifIcFH8EoGDA4kXAiUL4H0aJIR1AkfJHNBUqMnRgMExgnzgV7B0cbNFsujSipyl2v6/X3/j+EmBmPu6vn0+uu6qnpmSZIO3q4vBzdBhg6zu+8/ufP1Vb88Ne/Q9a7plHbmK/3j9XWiiEIWBwBiyNgccelfyh1EHo97mG9EbaRChb3aBzMMKkvhkkwR8DiCFgcAYsrDpNK8gN4iUWHoma9axq1ja3rbR4HY1vYRYsjYHEELK65k9Wq13lazzxvIxUszryCW/9Ct1K1Oc/bWAx4iy80nmIXLY6AxRGwuGFv+MMOb/gHQsDiCFgcAYsbNi961OM5b21jXjSaMUwSxDApEAIWR8DiCFjco2FSaV6uxTSUXnOAredLj9qWXuulgsURsDgCFkfA4hZ9+GzNOcAe5kuP2pYl66WCxTVX8Kg5wB7Ok4/aliXrpYLFEbA4AhZHwOKOR3Ve6Hyts17zTxf68eXNx3T73ff3w9qxrii76Dzdp3eFhQj4y5uP776/n6s23QiScYiApyzOILkm3eZF91qOxeMPP34ddPOjr4e2WbyGuSidrLAVfDS9vumyIA/v9pSUQvXWl7Z4DaMcg8MiYHEhAk4DpIPfEYQIeAqc8bHFNYp7zVVWmhdtgXnRIGB1BCyOgMWZz4vupWa9vdpT6rx43vYSKlic+bzoXjyc397itlPB4ghYHAGLI2BxxXnRredXPXSCRvGw7VzKMCgCFkfA4ghYXLdz0d7mElvY4rZTweKK86I9VEyN+nbeXr1Kt69uf3ZZ5lhc8f2XPN2nd4WFCPj26tXV7c+5atONIBmHCHjK4gySaxIl4HTQff7oq6dqXnQNi7nBHTs4phXsYdtLbYhSwWERsDgCFhci4DRAOvgdgdS86Gd0yXiL2x6igiOTmhft4aOk3tZLBYsjYHEELI6AxYW4ENpWcCG0Rb7dfJp/5tujm7OSKAHniX67+XRyfRYk4ygBJyfXZ9P/GY9uyxqiBHxyfZaiHd2WVYW4XnR6/CBjV217xpLlRKngsGINk/L9s8NjsMVrGOWK79EOvUnQXbTD8jUSJeA80TjpTnF20VOwXBOT60XXsP6ac4trjIz6evua5ZdE2UWHRcDiCFgcAYsznxft7fk1vLWZedEoMp8X7e35Htqw5jZSweIIWBwBiyNgcY86WUsO5t7e5PfWnhq9Ol/5sIoKFkfA4ghYHAGL6zYv2noOcK929vq/Nctpfb7Fa0sFizt6uLxMdzxfOcbDvOuttJlhUiAELI6AxRGwuEVTdko8vAlvMS/auqNk8dpSweIIWBwBiyNgcc3noluNOk+7hLfz7UuWQwWLK1Zwr2roNQ2l9fElrNuw5mtLBYsjYHEELI6Axbm4CIvFeWML1l/6YcF8HIyx2EWLI2BxBCzOvJO15sXARl2jw9vyc1SwOPMKtv4L9TA9yNvyc+ZvNmAsdtHiCFgcAYsb9uEz2OHDZ4EQsDgCFkfA4oZdo8PD9wN6axvX6EAzhkmCGCYFQsDiCFgcAYt7NEza4rxfPI8KBiK4v387/55/uj8/prvd7m63M13FUc2T7u/fnp9/Pcjp/Pxrr+fHdLfbXez3841pmubb3VUdg1NalSG1Pj+mPNGL/d6olNumzbbGRsz15oy713FbwK1REe2LDnbU3TFMGulput1jbgu4tUtMF/pF1r3o36lgYu4ir9p06O1+DK4NmKOvtYv93mKk1FDBeWY1+bU+Pxq7qgUAAONVvdkQ3OcPp/ON078/j23Jb+BM1gvmdOdoU9IQ8fnDaR7qwd1NoILFETBiS7vlLe6fJ3rRNTbdiwYAAAAAAAAAAAAAaPoP/Mqq8fn0/uYAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_data.query(\"`frame_id` == '{}'\".format(prefix+\"1\"))"
      ],
      "metadata": {
        "id": "2CFgKFHkrvxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Make a dictionary to look up gaze positions by frame_id\n",
        "meta_data.query(\"`frame_id` == '{}{}'\".format(prefix, 1))"
      ],
      "metadata": {
        "id": "iEcR6ogBr4K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gaze_list(data, frame_index, threshold=10):\n",
        "  sub_df = data.query(\"`frame_id` == '{}{}'\".format(prefix, frame_index+1))\n",
        "  sub_df = sub_df.groupby(np.arange(len(sub_df))//threshold).mean()\n",
        "  gaze_tups = list(zip(sub_df.gaze_position_x, sub_df.gaze_position_y))\n",
        "  rep_img = [frame_index for _ in range(len(gaze_tups))]\n",
        "  return zip(rep_img, gaze_tups)\n",
        "\n",
        "#get_gaze_list(meta_data, 0)\n",
        "gaze_dict = { str(frame_index) : get_gaze_list(frame_index) for frame_index in range(data_len)}"
      ],
      "metadata": {
        "id": "7ttD6W2iTXZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AtariDataset(torch.data.Dataset):\n",
        "    def __init__(self, captionsDir, imagesDir, pickleDir, captionToImageRatio=10, imageTransform=None, split='train', maxCaptionLength = 18):\n",
        "        # load pickle serializations for data grabbing\n",
        "        # THIS ASSUMES THAT PICKLE DATA IS AVAILABLE\n",
        "        # image data includes a list of captions for each image\n",
        "        self.captionsDir = captionsDir\n",
        "        self.imagesDir = imagesDir\n",
        "        if split == 'train':\n",
        "            # cub train data mappings\n",
        "            with open(os.path.join(pickleDir,'trainImagesCUB.pickle'),'rb') as obj:\n",
        "                self.imageData = pickle.load(obj,encoding='bytes')\n",
        "            with open(os.path.join(pickleDir,'CUBWordToIndex.pickle'),'rb') as obj:\n",
        "                self.wordToIndex = pickle.load(obj, encoding='bytes')\n",
        "            with open(os.path.join(pickleDir,'CUBIndexToWord.pickle'),'rb') as obj:\n",
        "                self.indexToWord = pickle.load(obj,encoding='bytes')\n",
        "            with open(os.path.join(pickleDir,'CUBTrainID.pickle'),'rb') as obj:\n",
        "                self.IDList = pickle.load(obj,encoding='bytes')\n",
        "        elif split == 'test' and os.path.basename(os.path.dirname(imagesDir))=='CUB_200_2011':\n",
        "            # cub test data mappings\n",
        "            with open(os.path.join(pickleDir, 'testImagesCUB.pickle'), 'rb') as obj:\n",
        "                self.imageData = pickle.load(obj, encoding='bytes')\n",
        "            with open(os.path.join(pickleDir, 'CUBWordToIndex.pickle'), 'rb') as obj:\n",
        "                self.wordToIndex = pickle.load(obj, encoding='bytes')\n",
        "            with open(os.path.join(pickleDir, 'CUBIndexToWord.pickle'), 'rb') as obj:\n",
        "                self.indexToWord = pickle.load(obj, encoding='bytes')\n",
        "            with open(os.path.join(pickleDir, 'CUBTestID.pickle'), 'rb') as obj:\n",
        "                self.IDList = pickle.load(obj, encoding='bytes')\n",
        "\n",
        "    def __len__(self):\n",
        "        # we will return the number of images\n",
        "        return len(self.imageData)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        img = Image.open(os.path.normpath(os.path.join(self.imagesDir,\"{}{}\".format(prefix, index+1)).replace('\\\\','/'))).convert('RGB')\n",
        "        #print(imgData[0])\n",
        "\n",
        "        # perform any transforms needed on the image\n",
        "        if imgData[2] is not None:\n",
        "            bbox=imgData[2]\n",
        "            # then bounding boxes, we have cub data\n",
        "            width,height = img.size\n",
        "            # bbox[0] is left x pixel, bbox[1] is the top y pixel of the bounding box\n",
        "            # bbox[2] is the width of the bounding box containing the bird\n",
        "            # bbox[3] is the height of the bounding box containing the bird\n",
        "            # we do not want to crop the image right to the bird\n",
        "            # instead, the paper stackgan mentions they crop bounding boxes of the birds\n",
        "            # # so that they have greater than 0.75 object-image size ratios\n",
        "            # # therefore, we pick the max width/height of the box and crop 0.25 more of what the dataset preparers\n",
        "            # # say to crop\n",
        "            r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "            center_x = int(bbox[0] + (bbox[2]/2))\n",
        "            center_y = int(bbox[1]+(bbox[3])/2)\n",
        "            # new bottom coordinate of the cropped image\n",
        "            y1 = np.maximum(0, center_y - r)\n",
        "            # new top coordinate of the cropped image\n",
        "            y2 = np.minimum(height, center_y + r)\n",
        "            # new left most x coordinate of the cropped image\n",
        "            x1 = np.maximum(0, center_x - r)\n",
        "            # new right most x coordinate of the cropped image\n",
        "            x2 = np.minimum(width, center_x + r)\n",
        "            img = img.crop([x1, y1, x2, y2])\n",
        "            # showing image here just for testing\n",
        "            #img.show()\n",
        "\n",
        "        if self.imageTransform is not None:\n",
        "            img = self.imageTransform(img)\n",
        "\n",
        "        # normalizing channels of the image\n",
        "        img = self.normalize(img)\n",
        "\n",
        "\n",
        "        # need to do just a tiny bit of work to the caption before returning\n",
        "        numWords = len(randCaption)\n",
        "        # just padding with zeroes\n",
        "        padded = np.zeros((self.maxCaptionLength,1))\n",
        "\n",
        "        # if the text is longer than 18, we need to rearrange the sentence so it makes sense with 18 tokens\n",
        "        # so, all 18 tokens should be in order\n",
        "\n",
        "        if numWords <= self.maxCaptionLength:\n",
        "            # no issues here, our caption is less than the max\n",
        "            padded[:numWords,0] = randCaption\n",
        "        else:\n",
        "            # grabbing indices of words as a list\n",
        "            indices = np.arange(numWords)\n",
        "            # shuffling the indices\n",
        "            np.random.shuffle(indices)\n",
        "            # limiting the indices\n",
        "            indices = indices[:self.maxCaptionLength]\n",
        "            # sorting the indices so the sentence order is preserved\n",
        "            indices = np.sort(indices)\n",
        "            # now just setting the pad based on the index\n",
        "            for index,wordIndex in enumerate(indices):\n",
        "                padded[index,0] = randCaption[wordIndex]\n",
        "\n",
        "        return img, padded, min(numWords,self.maxCaptionLength), classID"
      ],
      "metadata": {
        "id": "VTwcyJ6g60YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load combined.csv file\n",
        "df = pd.read_csv('/content/drive/MyDrive/erdos/ErdosBootcampProject/raw_data/combined.csv')\n",
        "\n",
        "gaze_x = df.gaze_position_x\n",
        "gaze_y = df.gaze_position_x\n",
        "frame_id = df.frame_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "P-7W9pTjZCmo",
        "outputId": "ab478747-da42-4b04-f287-980f16e097dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       frame_id  score  duration  unclipped_reward  action_int  \\\n",
              "0  RZ_4303947_1    NaN      2817                 0           4   \n",
              "1  RZ_4303947_1    NaN      2817                 0           4   \n",
              "2  RZ_4303947_1    NaN      2817                 0           4   \n",
              "3  RZ_4303947_1    NaN      2817                 0           4   \n",
              "4  RZ_4303947_1    NaN      2817                 0           4   \n",
              "\n",
              "   gaze_position_x  gaze_position_y     action_str        trial_id  \n",
              "0            79.81           118.15  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "1            79.69           119.38  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "2            79.67           120.15  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "3            79.64           121.30  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "4            79.60           121.65  PLAYER_A_LEFT  118_RZ_4303947  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07fe2dbd-9c51-4ab8-9bd3-9d456536843e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>frame_id</th>\n",
              "      <th>score</th>\n",
              "      <th>duration</th>\n",
              "      <th>unclipped_reward</th>\n",
              "      <th>action_int</th>\n",
              "      <th>gaze_position_x</th>\n",
              "      <th>gaze_position_y</th>\n",
              "      <th>action_str</th>\n",
              "      <th>trial_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.81</td>\n",
              "      <td>118.15</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.69</td>\n",
              "      <td>119.38</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.67</td>\n",
              "      <td>120.15</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.64</td>\n",
              "      <td>121.30</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.60</td>\n",
              "      <td>121.65</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07fe2dbd-9c51-4ab8-9bd3-9d456536843e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07fe2dbd-9c51-4ab8-9bd3-9d456536843e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07fe2dbd-9c51-4ab8-9bd3-9d456536843e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h,w = 210,160\n",
        "hidden_size = 256\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "K6cCGswb2uf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mask(torch.nn.Module):\n",
        "  def  __init__(self):\n",
        "      super().__init__()\n",
        "      self.MLP = torch.nn.Sequential(\n",
        "          torch.nn.Linear(in_features=1000, out_features =  64),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(in_features=65, out_features=h*w*1) #\n",
        "      )\n",
        "      self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "      def forward(self, random_vector, gaze_bias):\n",
        "      \"\"\"\n",
        "      Given a random vector of length 1000, this makes a learned weighted mask\n",
        "      Args:\n",
        "        - random_vector (Tensor): Our input random vector with shape (1000,)\n",
        "        - gaze_bias (Tensor): Our one-hot-encoded tensor with shape (1, h, w)\n",
        "      Returns:\n",
        "        - out (Tensor): Our learned weighted mask, which will be applied to our input image later\n",
        "      \"\"\"\n",
        "        #start we have x with shape (1000,)\n",
        "        #apply our fully connected layer\n",
        "        out = self.MLP(random_vector) #This should now have a shape of (h*w*1)\n",
        "        out = out.view() #unflatten\n",
        "        #apply gaze_bias to learned mask\n",
        "        out = out + gaze_bias\n",
        "        #apply sigmoid now to make values go between 0 and 1\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class CNN(torch.nn.Module):\n",
        "  def  __init__(self):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.learned_mask = Mask()\n",
        "\n",
        "      self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=100, kernel_size=3, stride=1)\n",
        "      self.relu = torch.nn.ReLU()\n",
        "      self.maxpool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "      self.flatten = torch.nn.Flatten()\n",
        "      self.MLP = torch.nn.Sequential(\n",
        "          torch.nn.Linear(in_features = 100*h*w, out_features=16),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(in_features=16, out_features=9)\n",
        "      )\n",
        "      self.final_activation = torch.nn.Softmax()\n",
        " \n",
        "  def forward(self, x, random_vector, mask):\n",
        "     \"\"\"\n",
        "    This applies our input image with the mask and then runs it through the rest of the CNN\n",
        "    Args:\n",
        "      - x (Tensor): Our input image with shape (3, h, w)\n",
        "      - random_vector (Tensor): Random vector used to make mask with shape (1000)\n",
        "      - gaze_bias (Tensor): Our one-hot-encoded tensor with shape (1, h, w)\n",
        "    Returns:\n",
        "      - prediction: Which action?\n",
        "    \"\"\"\n",
        "    learned_weight_mask = self.learned.mask(random_vector)\n",
        "\n",
        "    \n",
        "    #Apply the mask to the image to get initial input to CNN parts\n",
        "    out = torch.mul(x, learned_weight_mask) #shape (3,h,w)\n",
        "\n",
        "    out = self.conv1(out) #shape (100,h,w)\n",
        "    out = self.relu1(out)\n",
        "    out = self.maxpool1(out) #shape (100,h,w)\n",
        "\n",
        "    #apply the fully connected layers\n",
        "    out = self.flatten(out)\n",
        "\n",
        "    out = self.final_activation(self.MLP(out)) #this will get you probability vector with probs for each class\n",
        "    \n",
        "    #Output the class associated with the highest probability\n",
        "    return torch.argmax(out)"
      ],
      "metadata": {
        "id": "ZBejCk6g3Z-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = len()  #Number of epochs correspond to the number of images 17814\n",
        "training_data = None #TODO: load training data (images, gaze_coords)\n",
        "cnn_model = CNN()\n",
        "opt_cnn = torch.optim.SGD(cnn_model.parameters(), lr=1e-2, momentum=0.9)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#This is training\n",
        "for epoch in range(num_epochs):\n",
        "  for image, gaze_coords, y in training_data:\n",
        "    #Every step\n",
        "\n",
        "    #Grab the gaze coordinates to do one hot encoding\n",
        "    gaze_x, gaze_y = gaze_coords[0], gaze_coords[1]\n",
        "\n",
        "    #Make one hot encoded gaze_bias\n",
        "    gaze_bias = torch.zeros((1, h, w), requires_grad=False)\n",
        "    gaze_bias[:, gaze_y, gaze_x] = 1\n",
        "\n",
        "    #Make random vector that will be used to make mask\n",
        "    random_vector = torch.rand(1000, requires_grad=True)\n",
        "\n",
        "    #Zero gradients before calculating gradients of loss with respect to the image\n",
        "    opt_cnn.zero_grad()\n",
        "\n",
        "    #Pass image into cnn_model\n",
        "    y_hat = cnn_model(image, random_vector, gaze_bias)\n",
        "    \n",
        "    #Calculate loss \n",
        "    loss = loss_func(y_hat, y)\n",
        "\n",
        "    #Calcualte gradients with backprob\n",
        "    loss.backward()\n",
        "\n",
        "    #Step optimizer to update weights with new grads\n",
        "    opt_cnn.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "pK__zQu52wcN",
        "outputId": "f2cffed3-62e0-433c-b1a5-4e65afa70063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-564ca40e31f1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    num_epochs =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W0oWaOQSOm0X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}