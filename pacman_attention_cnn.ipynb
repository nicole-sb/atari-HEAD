{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pacman_attention_cnn",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicole-sb/atari-HEAD/blob/main/pacman_attention_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NJZ7nNC89hB",
        "outputId": "afe66994-27ad-4b65-c700-4e027d655a19"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ],
      "metadata": {
        "id": "EeYHyjcjpp6A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_path = \"/content/drive/MyDrive/ErdosBootcampProject\" "
      ],
      "metadata": {
        "id": "XKhRJ2Vcclte"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"{}/raw_data/highscore\".format(parent_path)"
      ],
      "metadata": {
        "id": "TuYsadRIplNO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tars = glob.glob(\"{}/*.tar.bz2\".format(train_path)) #get directories of tar files\n",
        "tars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_64I_SeczXfg",
        "outputId": "ccf8fa6a-35b4-4869-821b-24dd346c2490"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/ErdosBootcampProject/raw_data/highscore/118_RZ_4303947_Sep-01-17-15-39.tar.bz2',\n",
              " '/content/drive/MyDrive/ErdosBootcampProject/raw_data/highscore/593_RZ_5037271_Aug-05-15-35-12.tar.bz2']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xjf {tars[0]} #untar first trial '118_RZ_4303947"
      ],
      "metadata": {
        "id": "al_5EQEeza4W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_data = pd.read_csv(\"{}/raw_data/combined.csv\".format(parent_path))\n",
        "prefix = \"RZ_4303947_\"\n",
        "meta_data = meta_data[meta_data['frame_id'].str.contains(prefix)]"
      ],
      "metadata": {
        "id": "X2ox5XlE9pOx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter rows that have gaze positions outside of bounds (x > 161 and y > 211)\n",
        "meta_data = meta_data[(meta_data['gaze_position_x'] <= 161)]\n",
        "meta_data = meta_data[(meta_data['gaze_position_y'] <= 211)]\n",
        "data_len = int(meta_data.tail(1)['frame_id'].values.tolist()[0].split('_')[-1])\n"
      ],
      "metadata": {
        "id": "1JVqg7pDFSQS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_len = int(meta_data.tail(1)['frame_id'].values.tolist()[0].split('_')[-1])\n",
        "tar_name = tars[0].split('/')[-1].split('.tar')[0]\n",
        "\n",
        "def load_img(index):\n",
        "  return Image.open(\"{}/{}{}.png\".format(tar_name, prefix, index+1)).convert('RGB')\n",
        "\n",
        "#Read entire dataset into memory\n",
        "images = [np.array(load_img(i)) for i in range(data_len)]"
      ],
      "metadata": {
        "id": "wHl2VGnH9tBi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('{}/processed_data/images.pkl'.format(parent_path), 'wb') as f:\n",
        "  pickle.dump(images, f)"
      ],
      "metadata": {
        "id": "VxXhUDdlCUYs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('{}/processed_data/images.pkl'.format(parent_path), 'rb') as f:\n",
        "  images = pickle.load(f)"
      ],
      "metadata": {
        "id": "iE5WqZZ-bc1r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating training data and binning of gaze coordinates with only one pass through dataset"
      ],
      "metadata": {
        "id": "pUmlgDeebsMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = []\n",
        "prev_frame = meta_data.iloc[0]\n",
        "agg_list = []\n",
        "threshold = 10\n",
        "for curr_index, _ in enumerate(tqdm(range(len(meta_data)))) :\n",
        "  curr_frame = meta_data.iloc[curr_index]\n",
        "  curr_gaze_coords = (curr_frame['gaze_position_x'], curr_frame['gaze_position_y'] )\n",
        "  #First check if new frame, if so then must bin the rest that we left on\n",
        "  if curr_frame['frame_id'] != prev_frame['frame_id']:\n",
        "    if len(agg_list) > 0:\n",
        "      #Average gaze coords for aggregated list of rows\n",
        "      avg_gaze_coords = tuple(sum(y) / len(y) for y in zip(*agg_list))\n",
        "      #Make the training example and add it to training data\n",
        "      example = (int(prev_frame['frame_id'].split('_')[-1])-1, avg_gaze_coords[0], avg_gaze_coords[1], int(prev_frame['action_int']) )\n",
        "      training_data.append(example)\n",
        "      #reset and add the gaze coords for the new frame\n",
        "      agg_list = []\n",
        "    agg_list.append(curr_gaze_coords )\n",
        "    prev_frame = curr_frame\n",
        "    continue\n",
        "\n",
        "  if len(agg_list)+1 == threshold:\n",
        "    #First add the current frame's gaze coords before averaging\n",
        "    agg_list.append(curr_gaze_coords )\n",
        "    #Average the current bin's gaze values\n",
        "    avg_gaze_coords = tuple(sum(y) / len(y) for y in zip(*agg_list))\n",
        "    #Make training example and add to training, then reset\n",
        "    example = (int(curr_frame['frame_id'].split('_')[-1])-1, avg_gaze_coords[0], avg_gaze_coords[1], int(curr_frame['action_int']) )\n",
        "    training_data.append(example)\n",
        "    agg_list = []\n",
        "    prev_frame = curr_frame\n",
        "    continue\n",
        "  \n",
        "  #Otherwise we're in the same frame and don't need to bin yet, so we just add to agg_list and update prev\n",
        "  agg_list.append(curr_gaze_coords)\n",
        "  prev_frame = curr_frame\n",
        "\n",
        "if len(agg_list) > 0:\n",
        "  #Still have left over in agg_list after going through entire meta_data\n",
        "  #Average gaze coords for aggregated list of rows\n",
        "  avg_gaze_coords = tuple(sum(y) / len(y) for y in zip(*agg_list))\n",
        "  #Make the training example and add it to training data\n",
        "  example = (data_len - 1, avg_gaze_coords[0], avg_gaze_coords[1], int(meta_data.tail(1)['action_int']) )\n",
        "  training_data.append(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60sA3rH8PS3m",
        "outputId": "dba2f83d-d756-4519-cbe7-9e8055124521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 156843/879295 [00:33<01:56, 6179.43it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('{}/processed_data/X_train.pkl'.format(parent_path), 'wb') as f:\n",
        "  pickle.dump(training_data, f)"
      ],
      "metadata": {
        "id": "ghJfIa-qb0yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def get_gaze_list(data, frame_index, threshold=10):\n",
        "  sub_df = data.query(\"`frame_id` == '{}{}'\".format(prefix, frame_index+1))\n",
        "  sub_df = sub_df.groupby(np.arange(len(sub_df))//threshold).mean()\n",
        "  gaze_tups = list(zip(sub_df.gaze_position_x, sub_df.gaze_position_y))\n",
        "  rep_img = [frame_index for _ in range(len(gaze_tups))]\n",
        "  return zip(rep_img, gaze_tups)\n",
        "\n",
        "#get_gaze_list(meta_data, 0)\n",
        "gaze_dict = { str(frame_index) : get_gaze_list(frame_index) for frame_index in range(data_len)}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7ttD6W2iTXZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ErdosDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_path, train_path):\n",
        "        # load pickle serializations for data grabbing\n",
        "        with open(image_path, 'rb') as f:\n",
        "          self.images = pickle.load(f)\n",
        "\n",
        "        with open(train_path, 'rb') as f:\n",
        "          self.X_train = pickle.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        # we will return the number of bins\n",
        "        return len(self.X_train)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "      \"\"\"\n",
        "      Returns the training example for the given index.\n",
        "      Args:\n",
        "      - index (int): index of the example to grab\n",
        "      Returns:\n",
        "      - img (numpy arr): Frame image with shape (210, 160, 3)\n",
        "      - gaze_x (float): average x-coordinate for the given bin\n",
        "      - gaze_y (float): average y-coordinate for the given bin\n",
        "      - y (int): Integer value of the true class (action)\n",
        "      \"\"\"\n",
        "      img_idx, gaze_x, gaze_y, y = self.X_train[index] #Grab the data\n",
        "\n",
        "      #Grab the image associated with img_idx\n",
        "      img = self.images[img_idx]\n",
        "\n",
        "      return img, gaze_x, gaze_y, y"
      ],
      "metadata": {
        "id": "VTwcyJ6g60YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h,w = 210,160\n",
        "hidden_size = 256\n",
        "batch_size = 64\n",
        "to_print = False"
      ],
      "metadata": {
        "id": "K6cCGswb2uf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ErdosDataset('{}/processed_data/images.pkl'.format(parent_path), '{}/processed_data/X_train.pkl'.format(parent_path))\n",
        "train_data_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size, drop_last=True, shuffle=True, num_workers=8, pin_memory=True)"
      ],
      "metadata": {
        "id": "eldpft95CbbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Learn Weighted Mask "
      ],
      "metadata": {
        "id": "9JPM4ujTnKfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mask(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.MLP = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=1000, out_features= 64 ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=64, out_features=h*w*1)\n",
        "        )\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, z, gaze_bias):\n",
        "      \"\"\"\n",
        "      Given a random Tensor of shape (batch_size, 1000), this makes a learned weighted mask\n",
        "      Args:\n",
        "        - z (Tensor): Our input random vector with shape (batch_size, 1000)\n",
        "        - gaze_bias (Tensor): Our one-hot-encoded tensor with shape (batch_size, h, w)\n",
        "      Returns:\n",
        "        - out (Tensor): Our learned weighted mask, which will be applied to our input image later\n",
        "      \"\"\"\n",
        "      #Start we have z with shape (batch_size, 1000)\n",
        "      #Apply our fully connected layer\n",
        "      out = self.MLP(z) #This should now have a shape of (batch_size, h*w*1)\n",
        "      out = out.view((out.shape[0], 1, h, w)) #Unflatten, so this should now have a shape of (batch_size, h, w, 1)\n",
        "      \n",
        "      #Reshape gaze_biase from (batch_size, h, w) to (batch_size, 1, h, w) to match out shape\n",
        "      gaze_bias = gaze_bias.unsqueeze(1)\n",
        "\n",
        "      #Apply gaze_bias to learned mask\n",
        "      if to_print:\n",
        "        print(\"[Mask] Out shape: \", out.shape)\n",
        "        print(\"[Mask] Gaze Bias shape: \", gaze_bias.shape)\n",
        "      out = out + gaze_bias\n",
        "      #Apply sigmoid now to make sure values go between 0 and 1\n",
        "      return self.sigmoid(out) #This is the learned weight mask with gaze information"
      ],
      "metadata": {
        "id": "cY_NyZZznEJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "        self.learned_mask = Mask()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=100, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = torch.nn.ReLU()\n",
        "        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.MLP = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=100*h*w, out_features=16),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=16, out_features=10)\n",
        "        )\n",
        "        #self.final_activation = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self, x, z, gaze_bias):\n",
        "    \"\"\"\n",
        "    This applies our input image with the mask and then runs it through the rest of the CNN\n",
        "    Args:\n",
        "      - x (Tensor): Our input image with shape (batch_size, 3, h, w)\n",
        "      - z (Tensor): Random tensor used to make mask with shape (batch_size, 1000)\n",
        "      - gaze_bias (Tensor): Our one-hot-encoded tensor with shape (batch_size, h, w)\n",
        "    Returns:\n",
        "      - prediction: Which action?\n",
        "    \"\"\"\n",
        "\n",
        "    learned_weight_mask = self.learned_mask(z, gaze_bias)\n",
        "    if to_print:\n",
        "      print(\"[CNN] Mask shape: \", learned_weight_mask.shape)\n",
        "\n",
        "    #Apply the mask to the image to get initial input to CNN parts\n",
        "    out = torch.mul(x, learned_weight_mask) #Shape (batch_size, 3,h,w)\n",
        "    if to_print:\n",
        "      print(\"[CNN] Element mult shape: \", out.shape)\n",
        "\n",
        "    #Apply the convs etc.\n",
        "    out = self.conv1(out) #Shape (batch_size, 100, h, w)\n",
        "    if to_print:\n",
        "      print(\"[CNN] Conv1 shape: \", out.shape)\n",
        "    out = self.relu1(out)\n",
        "    out = self.maxpool1(out) #Shape (batch_size, 100, h, w)\n",
        "\n",
        "    #Flatten the image for the fully connected layers\n",
        "    out = self.flatten(out)\n",
        "    if to_print:\n",
        "      print(\"[CNN] Flatten shape: \", out.shape)\n",
        "\n",
        "    #Apply the Fully connected layers\n",
        "    out = self.MLP(out)\n",
        "    if to_print:\n",
        "      print(\"[CNN] MLP shape: \", out.shape)\n",
        "    #out = self.final_activation(out) #This will get you probability vector with probs for each class\n",
        "\n",
        "    #Then output the scores\n",
        "    return out"
      ],
      "metadata": {
        "id": "ZBejCk6g3Z-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cnn_model = CNN().to(device)\n",
        "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=1e-2, momentum=0.9)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "data_iterator = iter(train_data_loader)\n",
        "\n",
        "#This is training\n",
        "for epoch in range(num_epochs):\n",
        "  correct = 0\n",
        "  for step in tqdm(range(len(data_iterator))):\n",
        "    #Grab the train data\n",
        "    imgs, gazes_x, gazes_y, labels = data_iterator.next()\n",
        "    imgs = imgs.permute((0, 3, 1, 2)).to(device)\n",
        "    gazes_x = gazes_x.long().to(device)\n",
        "    gazes_y = gazes_y.long().to(device)\n",
        "    labels = labels.long().to(device)\n",
        "\n",
        "    if to_print:\n",
        "      print(\"Image shape: \", imgs.shape)\n",
        "      print(\"Gazes_X shape: \", gazes_x.shape)\n",
        "      print(\"Labels shape: \", labels.shape)\n",
        "\n",
        "    # #Make one-hot-encoded gaze_bias\n",
        "    with torch.no_grad():\n",
        "      gaze_bias = torch.zeros((batch_size, h, w), requires_grad=False).to(device)\n",
        "      for b in range(batch_size):\n",
        "        gaze_bias[b, gazes_y[b]-1, gazes_x[b]-1] = 1\n",
        "    if to_print:\n",
        "      print(\"Gaze bias shape: \", gaze_bias.shape)\n",
        "\n",
        "    # #Make random noise that will be used to make mask\n",
        "    z = torch.rand((batch_size, 1000), requires_grad=True).to(device)\n",
        "\n",
        "    # #Pass image into cnn_model\n",
        "    logits = cnn_model(imgs, z, gaze_bias)\n",
        "    if to_print:\n",
        "      print(\"[Train] logits shape: \", logits.shape)\n",
        "    # #Calculate loss \n",
        "    loss = loss_fn(logits, labels)\n",
        "\n",
        "    # #Zero gradients before calculating gradients of loss with respect to the image\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # #Calcualte gradients using backprob\n",
        "    loss.backward()\n",
        "\n",
        "    # #Step optimizer to update weights using the new grads\n",
        "    optimizer.step()\n",
        "    correct += ((torch.argmax(logits) == labels).float().sum()*(1/batch_size))\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "            loss  = loss.item()\n",
        "            print(f\"Loss: {loss:>7f}\")\n",
        "  acc = 100 * correct / len(data_iterator)\n",
        "  print(f\"Acc: {acc:>7f}\")"
      ],
      "metadata": {
        "id": "pK__zQu52wcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e51386f-75e1-42ad-84c0-6ae491e70a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "  0%|          | 1/1411 [00:33<13:07:32, 33.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 8.254302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_data, cnn_model, loss_fn):\n",
        "    size = len(test_data.dataset)\n",
        "    num_batches = len(test_data)\n",
        "    cnn_model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for image, gaze_coords, y in test_data:\n",
        "            image, gaze_coords, y = image.to(device), gaze_coords.to(device), y.to(device)\n",
        "            y_hat = cnn_model(image, gaze_coords, y)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "vCKwgK1vkXJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "id": "WlHALohro71x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W0oWaOQSOm0X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}